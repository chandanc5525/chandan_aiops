{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53b639",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'chandan_aiops (3.13.9) (Python 3.13.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/chandan_aiops/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import Data Manipulation Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Import Data Visualization Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Import Filter Warning Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder,OneHotEncoder,RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import Metrics for Regression and Classification\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Import Pipelines and Column Transformers\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Import OrderedDict for maintaining the order of columns in ColumnTransformer\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import scipy for statistical tests\n",
    "from scipy import stats, statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Import Logging\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO,\n",
    "                    format = '%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filemode = 'w',\n",
    "                    filename = 'model.log',force = True)\n",
    "\n",
    "# Import Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69539d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading Dataset Using Pandas Function\n",
    "# Url Taken from Github \n",
    "url = '  '\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.sample(frac = 1)  # Shuffle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee87751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Dataset Information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "stats = []\n",
    "\n",
    "# Descriptive statistics\n",
    "for col in df.columns:\n",
    "    if df[col].dtype != 'object':\n",
    "        numerical_stats = OrderedDict({\n",
    "            'Feature': col,\n",
    "            'Minimum': df[col].min(),\n",
    "            'Maximum': df[col].max(),\n",
    "            'Mean': df[col].mean(),\n",
    "            'Mode': df[col].mode()[0] if not df[col].mode().empty else None,\n",
    "            '25%': df[col].quantile(0.25),\n",
    "            '75%': df[col].quantile(0.75),\n",
    "            'IQR': df[col].quantile(0.75) - df[col].quantile(0.25),\n",
    "            'Standard Deviation': df[col].std(),\n",
    "            'Skewness': df[col].skew(),\n",
    "            'Kurtosis': df[col].kurt()\n",
    "        })\n",
    "        stats.append(numerical_stats)\n",
    "\n",
    "# Convert to DataFrame\n",
    "report = pd.DataFrame(stats)\n",
    "\n",
    "# Outlier Identification :\n",
    "outlier_label = []\n",
    "for col in report['Feature']:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    LW = Q1 - 1.5 * IQR   # LW : Lower Whisker Line\n",
    "    UW = Q3 + 1.5 * IQR   # UW : Upper Whisker Line\n",
    "    outliers = df[(df[col] < LW) | (df[col] > UW)]\n",
    "    if not outliers.empty:\n",
    "        outlier_label.append(\"Has Outliers\")\n",
    "    else:\n",
    "        outlier_label.append(\"No Outliers\")\n",
    "\n",
    "report[\"Outlier Comment\"] = outlier_label\n",
    "\n",
    "# Checking Report\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking BoxenPlot \n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.boxenplot(data= df)\n",
    "\n",
    "plt.xticks(rotation = 90)  \n",
    "plt.title(\"Boxen Plot of Numerical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Outliers with Median Statergy\n",
    "\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "    outlier_count = outliers.sum()\n",
    "\n",
    "    if outlier_count > 0:\n",
    "        replacement = df[col].median()  \n",
    "        df.loc[outliers, col] = replacement\n",
    "        print(f\"Replaced {outlier_count} outliers in '{col}' with median.\")\n",
    "    else:\n",
    "        print(f\"No outliers found in '{col}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create boxplot for all numerical columns\n",
    "sns.boxplot(data=df, orient='h', palette='Set2')\n",
    "\n",
    "# Set title\n",
    "plt.title('Boxplot After Outlier Treatment')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Dataset Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c24592",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ' '\n",
    "\n",
    "# Checking VIF:\n",
    "def calculate_vif(dataset):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['features'] = dataset.columns\n",
    "    vif['VIF_Values'] = [variance_inflation_factor(dataset.values,i) for i in range(dataset.shape[1])]\n",
    "    vif['VIF_Values'] = round(vif['VIF_Values'], 2)\n",
    "    vif = vif.sort_values(by = 'VIF_Values', ascending=False)\n",
    "    return (vif)\n",
    "\n",
    "calculate_vif(df.drop('target',axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b326acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the data\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Step 2: Determine number of components to retain 90% variance\n",
    "\n",
    "for i in range(1, df.shape[1] + 1):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X_scaled)\n",
    "    evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "    if evr[i - 1] >= 0.90:\n",
    "        pcs = i\n",
    "        break\n",
    "\n",
    "print(\"Explained Variance Ratio:\", evr)\n",
    "print(\"Number of components selected:\", pcs)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "\n",
    "pca = PCA(n_components=pcs)\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Create DataFrame\n",
    "\n",
    "pca_columns = [f'PC{j+1}' for j in range(pcs)]\n",
    "pca_df = pd.DataFrame(pca_data, columns=pca_columns)\n",
    "\n",
    "# Step 5: Join Target Column with PCA:\n",
    "\n",
    "pca_df = pca_df.join(df['target'], how = 'left')\n",
    "\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70334595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_split(data, tcol, testSize=0.3, randomState=3):\n",
    "    X = data.drop(tcol,axis=1)\n",
    "    y = data[tcol]\n",
    "    return train_test_split(X,y,test_size = testSize,random_state=randomState)\n",
    "\n",
    "def model_builder(model_name, model, data, t_col):\n",
    "    X_train,X_test,y_train,y_test = train_and_test_split(data,t_col)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "    result = [model_name, rmse, r2]\n",
    "    return result\n",
    "\n",
    "model_builder(model_name='LinearRegression',model=LinearRegression(),data=pca_df,t_col='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_models(data, t_col):\n",
    "    col_names=['Model Name','rmse','r2_score']\n",
    "    result = pd.DataFrame(columns = col_names)\n",
    "    result.loc[len(result)] = model_builder('Linear Regression',LinearRegression(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('Lasso',Lasso(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('Ridge',Ridge(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('Decision Tree',DecisionTreeRegressor(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('SVR',SVR(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('KNN',KNeighborsRegressor(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('Random Forest',RandomForestRegressor(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('Gradient Boost',GradientBoostingRegressor(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('ADA Boost',AdaBoostRegressor(),data,t_col)\n",
    "    result.loc[len(result)] = model_builder('XG Boost',XGBRegressor(),data,t_col)\n",
    "    return result.sort_values(by = 'r2_score', ascending=False)\n",
    "\n",
    "multiple_models(pca_df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54347db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(X, y , fold= 10):\n",
    "    score_LR = cross_val_score(LinearRegression(), X, y ,cv = fold)\n",
    "    score_LS = cross_val_score(Lasso(), X, y, cv = fold)\n",
    "    score_RD = cross_val_score(Ridge(), X, y, cv = fold)\n",
    "    score_DTR = cross_val_score(DecisionTreeRegressor(), X, y, cv = fold)\n",
    "    score_SVR = cross_val_score(SVR(), X, y ,cv = fold)\n",
    "    score_KNN = cross_val_score(KNeighborsRegressor(), X, y ,cv = fold)\n",
    "    score_RF = cross_val_score(RandomForestRegressor(), X, y ,cv = fold)\n",
    "    score_GB = cross_val_score(GradientBoostingRegressor(), X, y, cv = fold)\n",
    "    score_ADA = cross_val_score(AdaBoostRegressor(), X, y, cv = fold)\n",
    "    score_XG = cross_val_score(XGBRegressor(), X, y, cv = fold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_name = ['Linear Regression','Lasso','Ridge','DTR','SVR','KNN','Random Forest','Gradient Boost','ADA Boost','XG' ]\n",
    "    scores = [score_LR,score_LS,score_RD,score_DTR,score_SVR,score_KNN,score_RF,score_GB,score_ADA,score_XG]\n",
    "    result = []\n",
    "    for i in range(len(model_name)):\n",
    "        score_mean = np.mean(scores[i])\n",
    "        score_std = np.std(scores[i])\n",
    "        m_name = model_name[i]\n",
    "        temp = [m_name,score_mean,score_std]\n",
    "        result.append(temp)\n",
    "    k_fold_df = pd.DataFrame(result,columns = ['Model Name','CV Accuracy','CV STD'])\n",
    "    return k_fold_df.sort_values('CV Accuracy',ascending= False)\n",
    "\n",
    "\n",
    "k_fold_cv(pca_df.drop('target',axis=1), pca_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c587eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(X, y , fold = 10):\n",
    "    #Generally we dont do hyperparameter tuning for all the models, becuase if we train lasso and ridge at max it will give.\n",
    "    #Parameters Grid for tuning\n",
    "    param_LAS = {'alpha' : [1e-15, 1e-13, 1e-11, 1e-9,1e-7, 1e-5,1e-3, 1e-1, 0,1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,200,300,400,500]}\n",
    "    param_KNN = {'n_neighbors' : [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]}\n",
    "    param_DTR = {'max_depth' : [3,5,7,9,10,12,14,16] , 'max_features' : ['auto', 'log2', 'sqrt', 2,3,4,5,6]}\n",
    "    param_SVR = {'gamma' : ['scale' , 'auto'], 'C' : [0.5 , 1]}\n",
    "    param_ADB = {'learning_rate' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}\n",
    "    param_GB = {'alpha' : [0.1,0.3,0.5,0.9]}\n",
    "    param_XGB = {'eta' : [0.1,0.2,0.3,0.4,0.5], 'max_depth' : [3,5,7,9,10,12,14,15,16], 'gamma' : [0,10,20,30,40,50,60,70,80,90,100,200,300,400,500], 'reg_lambda' : [0,1]}\n",
    "    param_RF = {'max_depth' : [3,5,7,8,9,10,12,14,16], 'max_features' : ['auto', 'log2', 'sqrt', 2,3,4,5,6]}\n",
    "#hyperparameter Tuning\n",
    "    tune_LAS = GridSearchCV(Lasso(), param_LAS, cv = fold)\n",
    "    tune_RID = GridSearchCV(Ridge(), param_LAS , cv = fold)\n",
    "    tune_KNN = GridSearchCV(KNeighborsRegressor() , param_KNN , cv = fold)\n",
    "    tune_DT = GridSearchCV(DecisionTreeRegressor(), param_DTR , cv = fold)\n",
    "    tune_SVR = GridSearchCV(SVR(), param_SVR , cv = fold)\n",
    "    tune_ADB = GridSearchCV(AdaBoostRegressor() , param_ADB , cv = fold)\n",
    "    tune_GB = GridSearchCV(GradientBoostingRegressor() , param_GB , cv = fold)\n",
    "    tune_XGB = GridSearchCV(XGBRegressor() , param_XGB , cv = fold)\n",
    "    tune_RF = GridSearchCV(RandomForestRegressor() , param_RF , cv = fold)\n",
    "    \n",
    "    #Fitting X and Y\n",
    "    tune_LAS.fit(X,y)\n",
    "    tune_RID.fit(X,y)\n",
    "    tune_KNN.fit(X,y)\n",
    "    tune_DT.fit(X,y)\n",
    "    tune_SVR.fit(X,y)\n",
    "    tune_ADB.fit(X,y)\n",
    "    tune_GB.fit(X,y)\n",
    "    tune_XGB.fit(X,y)\n",
    "    tune_RF.fit(X,y)\n",
    "    \n",
    "    tune = [tune_LAS, tune_RID, tune_KNN, tune_DT, tune_SVR, tune_ADB, tune_GB, tune_XGB, tune_RF]\n",
    "    models = ['Lasso', 'Ridge', 'KNN' , 'DTR', 'SVR' , 'ADBR', 'GBR', 'XGBR' , 'RFR']\n",
    "    \n",
    "    for i in range(len(tune)):\n",
    "        print('models:', models[i])\n",
    "        print('best parameters :', tune[i].best_params_)\n",
    "\n",
    "    \n",
    "tuning(pca_df.drop('target',axis=1), pca_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a336938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_post_hpt(X,y, fold = 5):\n",
    "    score_LR = cross_val_score(LinearRegression(), X, y, cv = fold)\n",
    "    score_LS = cross_val_score(Lasso(alpha = 0.1), X, y ,cv = fold)\n",
    "    score_RD = cross_val_score(Ridge(alpha = 6 ), X , y , cv = fold)\n",
    "    score_DTR = cross_val_score(DecisionTreeRegressor(max_depth = 16), X , y , cv = fold)\n",
    "    score_SVR= cross_val_score(SVR(C = 1), X , y , cv = fold)\n",
    "    score_RandomForest = cross_val_score(RandomForestRegressor(max_depth = 14, max_features = 4), X , y , cv = fold)\n",
    "    score_KNN = cross_val_score(KNeighborsRegressor(n_neighbors = 4), X , y , cv = fold)\n",
    "    score_GBoost = cross_val_score(GradientBoostingRegressor(alpha = 0.9), X , y , cv = fold)\n",
    "    score_XGBoost = cross_val_score(XGBRegressor(eta = 0.2,max_depth = 5, reg_lambda = 0, gamma = 0), X , y , cv = fold)\n",
    "    score_AdaBoost = cross_val_score(AdaBoostRegressor(learning_rate = 1), X , y , cv = fold)\n",
    "    \n",
    "    \n",
    "    model_name = ['Linear Regression', 'Lasso', 'Ridge', 'DTR' , 'SVR' , 'Random Forest' , 'KNN', 'Gboost' , 'XGBoost' , 'AdaBoost']\n",
    "    scores = [score_LR, score_LS, score_RD, score_DTR, score_SVR,score_RandomForest, score_KNN, score_GBoost, score_XGBoost, score_AdaBoost ]\n",
    "    result = []\n",
    "    for i in range(len(model_name)):\n",
    "        score_mean = np.mean(scores[i])\n",
    "        score_std = np.std(scores[i])\n",
    "        m_name = model_name[i]\n",
    "        temp = [m_name , score_mean , score_std]\n",
    "        result.append(temp)\n",
    "    k_fold_df = pd.DataFrame(result , columns = ['Model Name' , 'CV Accuracy' , 'CV STD'])\n",
    "    return k_fold_df.sort_values('CV Accuracy', ascending = False)\n",
    "\n",
    "\n",
    "cv_post_hpt(pca_df.drop('target',axis=1), pca_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66862c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Learning Rate \n",
    "# List of models\n",
    "models = {\n",
    "    'Lasso': Lasso(alpha=1),\n",
    "    'Ridge': Ridge(alpha=1),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "    'DTR': DecisionTreeRegressor(max_depth=5),\n",
    "    'SVR': SVR(C=1, gamma='scale'),\n",
    "    'ADBR': AdaBoostRegressor(learning_rate=1),\n",
    "    'GBR': GradientBoostingRegressor(alpha=0.1),\n",
    "    'XGBR': XGBRegressor(eta=0.1, max_depth=3, verbosity=0),\n",
    "    'RFR': RandomForestRegressor(max_depth=5, max_features='sqrt')\n",
    "}\n",
    "\n",
    "X = pca_df.drop('target', axis=1)\n",
    "y = pca_df['target']\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(18, 20))\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', label='Cross-validation score')\n",
    "    plt.title(f'Learning Curve: {name}')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('RÂ² Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chandan_aiops (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
